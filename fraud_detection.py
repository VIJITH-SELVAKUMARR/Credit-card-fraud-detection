# -*- coding: utf-8 -*-
"""fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PZWoTLUm2QmIxsy2HryASzoiF6NOGHa1
"""

import numpy as np
import pandas as pd

# Read the data
data_set =pd.read_csv('/content/sample_data/Fraud.csv')
# Shape the data
data_set.shape

# Get first 1000 rows of the data
data_set.head(1000)

# Get last 1000 rows of the data
data_set.head(1000)

# Check for null values
data_set.isnull().values.any()

# collect information about the Data set
data_set.info()

# Count the occurrences of each unique value in the 'isFraud' column
counts = data_set['isFraud'].value_counts()

# Extract the counts for legitimate and fraudulent transactions
legit = counts.get(0, 0)  # Get the count for '0' (legitimate transactions), default to 0 if not found
fraud = counts.get(1, 0)  # Get the count for '1' (fraudulent transactions), default to 0 if not found

# Calculate the percentage of legitimate and fraudulent transactions
total_transactions = legit + fraud
legit_percent = (legit / total_transactions) * 100
fraud_percent = (fraud / total_transactions) * 100

# Print the results
print("Number of Legit transactions: ", legit)
print("Number of Fraud transactions: ", fraud)
print("Percentage of Legit transactions: {:.4f} %".format(legit_percent))
print("Percentage of Fraud transactions: {:.4f} %".format(fraud_percent))

# Drop rows with missing values in the 'nameDest' column
data_set.dropna(subset=['nameDest'], inplace=True)

# Merchants
x = data_set[data_set['nameDest'].str.contains('M')]
x.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram of transaction amounts
plt.figure(figsize=(10, 6))
sns.histplot(data_set['amount'], bins=50, kde=True)
plt.title('Distribution of Transaction Amounts')
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.show()

# Pie chart of fraudulent transactions
plt.figure(figsize=(8, 8))
data_set['isFraud'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['lightcoral', 'lightskyblue'])
plt.title('Fraudulent Transactions')
plt.ylabel('')
plt.show()

# Count plot of transaction types for fraudulent transactions
plt.figure(figsize=(10, 6))
sns.countplot(data=data_set[data_set['isFraud'] == 1], x='type')
plt.title('Transaction Types for Fraudulent Transactions')
plt.xlabel('Transaction Type')
plt.ylabel('Count')
plt.show()

# Scatter plot of transaction amount vs. old balance for fraudulent transactions
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data_set[data_set['isFraud'] == 1], x='oldbalanceOrg', y='amount')
plt.title('Transaction Amount vs. Old Balance for Fraudulent Transactions')
plt.xlabel('Old Balance')
plt.ylabel('Amount')
plt.show()

# Compute the correlation matrix
corr = data_set.corr()

# Set up the matplotlib figure
plt.figure(figsize=(10, 6))

# Plot the heatmap
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")

# Add title
plt.title('Correlation Heatmap')

# Show plot
plt.show()



"""Here, it is seen that newbalanceDest and oldbalanceDest are highly correlated"""

new_data_set = data_set.copy()
new_data_set.head()

# finding attributes that are of data type 'object'
objList = [col for col in new_data_set.columns if new_data_set[col].dtype == 'object']
print(objList)

#Label Encoding for object to numeric conversion
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for feat in objList:
    new_data_set[feat] = le.fit_transform(new_data_set[feat].astype(str))

print (new_data_set.info())

new_data_set.head()

# Import library for VIF (VARIANCE INFLATION FACTOR)

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(data_set):

    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = data_set.columns
    vif["VIF"] = [variance_inflation_factor(data_set.values, i) for i in range(data_set.shape[1])]

    return(vif)

calc_vif(new_data_set)

new_data_set['Actual_amount_orig'] = new_data_set.apply(lambda x: x['oldbalanceOrg'] - x['newbalanceOrig'],axis=1)
new_data_set['Actual_amount_dest'] = new_data_set.apply(lambda x: x['oldbalanceDest'] - x['newbalanceDest'],axis=1)
new_data_set['TransactionPath'] = new_data_set.apply(lambda x: x['nameOrig'] + x['nameDest'],axis=1)

#Dropping columns
new_data_set = new_data_set.drop(['oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest','step','nameOrig','nameDest'],axis=1)

calc_vif(new_data_set)

# Calculate the correlation matrix
corr = new_data_set.corr()

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(corr, annot=True)
plt.title('Correlation Heatmap')
plt.show()

# Perform Scaling
scaler = StandardScaler()
new_data_set["NormalizedAmount"] = scaler.fit_transform(new_data_set["amount"].values.reshape(-1, 1))
new_data_set.drop(["amount"], inplace= True, axis= 1)

Y = new_data_set["isFraud"]
X = new_data_set.drop(["isFraud"], axis= 1)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Create the decision tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
dt_classifier.fit(X_train, y_train)

# Predict the labels for the test set
y_pred = dt_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the evaluation results
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Assuming 'new_data_set' contains the preprocessed data with features and target variable
X = new_data_set.drop(columns=['isFraud'])  # Features
y = new_data_set['isFraud']  # Target variable

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the Random Forest classifier on the training data
rf_classifier.fit(X_train, y_train)

# Predict the target variable on the testing data
y_pred = rf_classifier.predict(X_test)

# Evaluate the performance of the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming X contains the features and y contains the target variable (isFraud)
# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initializing the Decision Tree classifier
clf = DecisionTreeClassifier(random_state=42)

# Training the classifier on the training data
clf.fit(X_train, y_train)

# Making predictions on the testing data
y_pred = clf.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Printing the results
print("Accuracy Score:", accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)